// Copyright (c) 2023 Climate Interactive / New Venture Fund

import type { ComparisonConfig } from '../config/comparison-config'
import type { ComparisonReport, ComparisonSummary, ComparisonTestSummary } from './comparison-report-types'

/**
 * Convert a full `ComparisonReport` to a simplified `ComparisonSummary` that includes
 * the minimum set of fields needed to keep the file smaller when there are many
 * reported differences.  This only includes comparison results for which there
 * is a non-zero `maxDiff` value.
 *
 * @param comparisonReport The full comparison report.
 * @return The terse summary.
 */
export function comparisonSummaryFromReport(comparisonReport: ComparisonReport): ComparisonSummary {
  const terseSummaries: ComparisonTestSummary[] = []

  for (const r of comparisonReport.testReports) {
    if (r.diffReport === undefined) {
      // The test was skipped; add a summary with undefined diff values
      terseSummaries.push({
        s: r.scenarioKey,
        d: r.datasetKey
      })
    } else if (r.diffReport?.validity === 'both' && r.diffReport.maxDiff > 0) {
      // The test produced a non-zero `maxDiff`, so include the summary
      let maxDiffRelativeToBaseline: number | undefined
      let avgDiffRelativeToBaseline: number | undefined
      if (r.baselineDiffReport) {
        // Calculate relative values when the baseline diff report is available
        // XXX: In the case where the baseline diff is zero, we need to avoid division
        // by zero.  We could use a special value like NaN or null for this case, but
        // it's simpler to instead use a small non-zero value for the denominator, which
        // will have the effect of making the relative value very large.  This is close
        // to the desired behavior, since we want to flag cases where there is a difference
        // between the two datasets for a given scenario but not for the baseline scenario.
        const epsilon = 0.000001
        const baselineMaxDiff = r.baselineDiffReport.maxDiff !== 0 ? r.baselineDiffReport.maxDiff : epsilon
        const baselineAvgDiff = r.baselineDiffReport.avgDiff !== 0 ? r.baselineDiffReport.avgDiff : epsilon
        maxDiffRelativeToBaseline = r.diffReport.maxDiff / baselineMaxDiff
        avgDiffRelativeToBaseline = r.diffReport.avgDiff / baselineAvgDiff
      } else {
        // When the baseline diff report is not available, it means that this is a report
        // for the baseline scenario.  We will use special relative values for this case.
        const baselineRelativeDiff = (x: number) => {
          if (x === 0) {
            // When the baseline diff is zero, set the relative value to zero so that
            // it falls into the "green" bucket
            return 0
          } else {
            // When the baseline diff is non-zero, set the relative value to 1 (meaning
            // that the difference is the same as itself); this will put it into the
            // "yellow" bucket
            return 1
          }
        }
        maxDiffRelativeToBaseline = baselineRelativeDiff(r.diffReport.maxDiff)
        avgDiffRelativeToBaseline = baselineRelativeDiff(r.diffReport.avgDiff)
      }
      terseSummaries.push({
        s: r.scenarioKey,
        d: r.datasetKey,
        md: r.diffReport.maxDiff,
        ad: r.diffReport.avgDiff,
        mdb: maxDiffRelativeToBaseline,
        adb: avgDiffRelativeToBaseline
      })
    }
  }

  return {
    testSummaries: terseSummaries,
    perfReportL: comparisonReport.perfReportL,
    perfReportR: comparisonReport.perfReportR
  }
}

/**
 * Restore the full set of summaries that was generated by `runComparison` (one summary
 * for each scenario/dataset pairing based on the `ComparisonConfig`).
 *
 * @param comparisonConfig The config that contains the scenarios used to reconstruct the
 * full set of summaries.
 * @param terseSummaries The set of terse summaries for comparisons that produced a
 * non-zero `maxDiff` result.
 */
export function restoreFromTerseSummaries(
  comparisonConfig: ComparisonConfig,
  terseSummaries: ComparisonTestSummary[]
): ComparisonTestSummary[] {
  // Put the provided summaries in a map for faster lookup
  const existingSummaries: Map<string, ComparisonTestSummary> = new Map()
  for (const summary of terseSummaries) {
    const key = `${summary.s}::${summary.d}`
    existingSummaries.set(key, summary)
  }

  // Get the full set of scenario/dataset pairs
  const allTestSummaries: ComparisonTestSummary[] = []
  for (const scenario of comparisonConfig.scenarios.getAllScenarios()) {
    const datasetKeys = comparisonConfig.datasets.getDatasetKeysForScenario(scenario)
    for (const datasetKey of datasetKeys) {
      const key = `${scenario.key}::${datasetKey}`
      const existingSummary = existingSummaries.get(key)
      if (existingSummary) {
        // We have a summary in the array that was passed in, which means the
        // `maxDiff` was undefined (meaning the comparison was skipped) or was
        // non-zero (meaning it had a non-zero difference), so preserve all fields
        allTestSummaries.push(existingSummary)
      } else {
        // We don't have a summary in the array that was passed in, which means
        // the comparison produced no difference, so use zero
        allTestSummaries.push({
          s: scenario.key,
          d: datasetKey,
          md: 0,
          ad: 0,
          mdb: 0,
          adb: 0
        })
      }
    }
  }

  return allTestSummaries
}
